{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Spark\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DataFrame Creation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame Creation on Spark\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.5| String|2024-01-01|2024-02-05 12:00:00|\n",
      "|  2|3.2|String2|2024-02-10|2024-02-03 11:00:00|\n",
      "|  3|4.6|String3|2024-03-21|2024-02-01 09:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Simple Spark DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    Row(a = 1, b = 2.5, c = 'String', d = date(2024, 1, 1), e = datetime(2024, 2, 5, 12, 0)),\n",
    "    Row(a = 2, b = 3.2, c = 'String2', d = date(2024, 2, 10), e = datetime(2024, 2, 3, 11, 0)),\n",
    "    Row(a = 3, b = 4.6, c = 'String3', d = date(2024, 3, 21), e = datetime(2024, 2, 1, 9, 0))\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.5|string1|2024-01-01|2024-01-02 09:00:00|\n",
      "|  2|6.6|string2|2024-02-05|2024-06-07 11:00:00|\n",
      "|  3|7.9|string3|2024-07-04|2024-04-05 00:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a Spark DataFrame based on a pandas DF\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a':[1,2,3],\n",
    "    'b':[2.5,6.6,7.9],\n",
    "    'c':[\"string1\", \"string2\", \"string3\"],\n",
    "    'd':[date(2024, 1, 1), date(2024, 2, 5), date(2024, 7, 4)],\n",
    "    'e':[datetime(2024, 1, 2, 9, 0), datetime(2024, 6, 7, 11, 0), datetime(2024, 4, 5, 0, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#See the Types of data in the dataframes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Viewing Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.5|string1|2024-01-01|2024-01-02 09:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vizualizing Specific Rows\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.5</td><td>string1</td><td>2024-01-01</td><td>2024-01-02 09:00:00</td></tr>\n",
       "<tr><td>2</td><td>6.6</td><td>string2</td><td>2024-02-05</td><td>2024-06-07 11:00:00</td></tr>\n",
       "<tr><td>3</td><td>7.9</td><td>string3</td><td>2024-07-04</td><td>2024-04-05 00:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.5|string1|2024-01-01|2024-01-02 09:00:00|\n",
       "|  2|6.6|string2|2024-02-05|2024-06-07 11:00:00|\n",
       "|  3|7.9|string3|2024-07-04|2024-04-05 00:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enabeling eagerEval for the eager evaluation of Pyspark DF in notebooks.\n",
    "#With this configuration we can have a more familiar vizualization of the table\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.5                 \n",
      " c   | string1             \n",
      " d   | 2024-01-01          \n",
      " e   | 2024-01-02 09:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 6.6                 \n",
      " c   | string2             \n",
      " d   | 2024-02-05          \n",
      " e   | 2024-06-07 11:00:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To have a vertical vizualization of the rows we can do:\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see the columns names we can do as follows:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------------+-------+\n",
      "|summary|  a|                 b|      c|\n",
      "+-------+---+------------------+-------+\n",
      "|  count|  3|                 3|      3|\n",
      "|   mean|2.0| 5.666666666666667|   NULL|\n",
      "| stddev|1.0|2.8183919765237295|   NULL|\n",
      "|    min|  1|               2.5|string1|\n",
      "|    max|  3|               7.9|string3|\n",
      "+-------+---+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To have a more Statistic view of the data we can use the descibe method\n",
    "df.select('a', 'b', 'c').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=2, b=6.6, c='string2', d=datetime.date(2024, 2, 5), e=datetime.datetime(2024, 6, 7, 11, 0)),\n",
       " Row(a=3, b=7.9, c='string3', d=datetime.date(2024, 7, 4), e=datetime.datetime(2024, 4, 5, 0, 0))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collecting data to the driver side as the local data in python:\n",
    "df.collect()\n",
    "#This is dangerous because if the data is too large it can cause a memory issue\n",
    "#For that we can use the take or tail method to collect specific data\n",
    "df.take(2)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.5</td><td>string1</td><td>2024-01-01</td><td>2024-01-02 09:00:00</td></tr>\n",
       "<tr><td>2</td><td>6.6</td><td>string2</td><td>2024-02-05</td><td>2024-06-07 11:00:00</td></tr>\n",
       "<tr><td>3</td><td>7.9</td><td>string3</td><td>2024-07-04</td><td>2024-04-05 00:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also convert a PySpark dataframe into a pandas one, but the same rule\n",
    "#we saw above can affect us here, as the memory problem.\n",
    "df.toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Operations With Columns***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operations with columns\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|string1|\n",
      "|string2|\n",
      "|string3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Returning another dataframe based on the original, using .select()\n",
    "df.select(df.c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.5|string1|2024-01-01|2024-01-02 09:00:00|STRING1|\n",
      "|  2|6.6|string2|2024-02-05|2024-06-07 11:00:00|STRING2|\n",
      "|  3|7.9|string3|2024-07-04|2024-04-05 00:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can add a new Column instance, with .withColumn\n",
    "df.withColumn('upper_c', upper(df.c)).show() # type: ignore\n",
    "#To add this change to the dataframe we have to save the operation with:\n",
    "    # df = df.withColumn('upper_c', upper(df.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  2|6.6|string2|2024-02-05|2024-06-07 11:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter rows, with .filter\n",
    "#Here we dont have the column we created above because we didnt save it\n",
    "df.filter(df.a == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Grouping Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>color</th><th>fruit</th><th>v1</th><th>v2</th></tr>\n",
       "<tr><td>red</td><td>banana</td><td>1</td><td>10</td></tr>\n",
       "<tr><td>blue</td><td>banana</td><td>2</td><td>20</td></tr>\n",
       "<tr><td>red</td><td>carrot</td><td>3</td><td>30</td></tr>\n",
       "<tr><td>blue</td><td>grape</td><td>4</td><td>40</td></tr>\n",
       "<tr><td>red</td><td>carrot</td><td>5</td><td>50</td></tr>\n",
       "<tr><td>black</td><td>carrot</td><td>6</td><td>60</td></tr>\n",
       "<tr><td>red</td><td>banana</td><td>7</td><td>70</td></tr>\n",
       "<tr><td>red</td><td>grape</td><td>8</td><td>80</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+------+---+---+\n",
       "|color| fruit| v1| v2|\n",
       "+-----+------+---+---+\n",
       "|  red|banana|  1| 10|\n",
       "| blue|banana|  2| 20|\n",
       "|  red|carrot|  3| 30|\n",
       "| blue| grape|  4| 40|\n",
       "|  red|carrot|  5| 50|\n",
       "|black|carrot|  6| 60|\n",
       "|  red|banana|  7| 70|\n",
       "|  red| grape|  8| 80|\n",
       "+-----+------+---+---+"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Groupby and Getting the avg\n",
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #You can apply Python functions to the dataframe\n",
    "# def plus_mean(pandas_df):\n",
    "#     return pandas_df.assign(v1= pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "# df.groupby('color').applyInPandas(plus_mean, schema = df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Getting Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data with CSV\n",
    "df.write.csv('foo.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  6| 60|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|banana|  7| 70|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  3| 30|\n",
      "|  red|carrot|  5| 50|\n",
      "|  red|banana|  1| 10|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('foo.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  6| 60|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  5| 50|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|banana|  7| 70|\n",
      "|  red|banana|  1| 10|\n",
      "|  red|carrot|  3| 30|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Parquet\n",
    "df.write.parquet('bar.parquet')\n",
    "spark.read.parquet('bar.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "|black|carrot|  6| 60|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|banana|  1| 10|\n",
      "|  red|carrot|  5| 50|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  3| 30|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ORC\n",
    "df.write.orc('zoo.orc')\n",
    "spark.read.orc('zoo.orc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Working with SQL***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf\n",
    "# @pandas_udf(\"integer\")\n",
    "# def add_one(s: pd.Series) -> pd.Series:\n",
    "#     return s + 1\n",
    "\n",
    "# spark.udf.register(\"add_one\", add_one)\n",
    "# spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import expr\n",
    "\n",
    "# df.selectExpr('add_one(v1)').show()\n",
    "# df.select(expr('count(*)') > 0).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
