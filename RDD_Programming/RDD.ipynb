{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Resilient Distributed Dataset (RDD)*\n",
    "\n",
    "* Is a collection of element partioned across the nodes of the cluster that can be operated on in parallel;\n",
    "* RDD are created by starting with a file in the Hadoop file system, or an existing Scala collection;\n",
    "* It can also recover automatically from node failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a SparkContext object, which tells Spark how to access a cluster\n",
    "#To build a SparkContext you first need a SparkConf that contains information about\n",
    "#our application\n",
    "conf = SparkConf().setAppName(\"RDD\").setMaster(\"local\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To create a RDD we can do a parallelizing of an existing collection in our driver program, or referencing a dataset in an external storage system, like the Hadoop environment*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelize Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6]\n",
    "distData = sc.parallelize(data) \n",
    "#This takes our collection and copies it to form a distributed dataset that can be\n",
    "#operated in parallel\n",
    "#You can also set the number of partitions the data is cut into, sc.parallelize(data, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"/Users/leoareias/Documents/Data_Engineering/RDD_Programming/test.txt\")\n",
    "#Here we use the sc.textFile to create an RDD where each element of the RDD represents a line in the text file\n",
    "#sc meaning SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "#The map transformation applies the provided function to each element of the RDD\n",
    "#The reduce action aggregates the elements of the RDD using the specified binary operator\n",
    "#Combining both the code calculates the length of each line and then sums these lengths \n",
    "#to get the total number of characters in the text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving and Loading SequenceFiles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1,5)).map(lambda x: (x, \"a\" * x))\n",
    "#Here we create a parallelized collection (1, 2, 3, 4), and then aplly a map function\n",
    "#to it multipling the number we are on by the letter a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsSequenceFile(\"/Users/leoareias/Documents/Data_Engineering/RDD_Programming/output_squence_file\")\n",
    "#Saving as a SequenceFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa'), (4, 'aaaa')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.sequenceFile(\"/Users/leoareias/Documents/Data_Engineering/RDD_Programming/output_squence_file\").collect())\n",
    "#Collecting the result we got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    "\n",
    "*There are 2 types of operations:*\n",
    "1. Transformations, which create a new dataset from an existing one. Example: Map\n",
    "\n",
    "2. Actions, which return a value to the driver program after running a computation on the dataset. Exemple: Reduce\n",
    "\n",
    "* To persist a function in memory to latter be used we can use the .persist() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Functions to Spark\n",
    "\n",
    "1. Lambda Expressions, for simple functions that can be writtes as an expression;\n",
    "\n",
    "2. Local defs inside the function calling into spark;\n",
    "\n",
    "3. Top-level function in a module;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "    \n",
    "rdd = sc.textFile(\"/Users/leoareias/Documents/Data_Engineering/RDD_Programming/test.txt\").map(myFunc)\n",
    "rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
